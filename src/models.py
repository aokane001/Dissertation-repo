# -*- coding: utf-8 -*-
"""Commented models.py file from TFGrid.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nR6qPBCqAGQxDVijbY-8AzVarQMqLk3_
"""

"""
Copyright (C) 2020 NVIDIA Corporation.  All rights reserved.
Licensed under the NVIDIA Source Code License. See LICENSE at https://github.com/nv-tlabs/lift-splat-shoot.
Authors: Jonah Philion and Sanja Fidler
"""

import torch
from torch import nn #need this for creating a neural network class
import torch.nn.functional as F #this is used for functions like relu
from efficientnet_pytorch import EfficientNet #might need to pip install this package first - will use this to get a pretrained efficient net for image backbone
from torchvision.models.resnet import resnet18, resnet34, resnet50 #these are used in TFGrid for Transformer part - also for BevEncode
                                                                   #versions to create semantic grids
import numpy as np
import math

#fixing relative import
from tools import gen_dx_bx, cumsum_trick, QuickCumsum #take in these functions from the tools.py file
                                                        #gen_dx_bx gives the grid spacings for the semantic grid produced
                                                        #cumsum_trick - this is used in the BEV pooling step-as described in LSS paper
                                                        #QuickCumSum - also has to do with cumulative sum for pooling (will need to see
                                                        #how this can be replaced
                                                        #by BEVFusion pooling operation if possible)

###############################################
################## Transfuser #################
###############################################

#All of this stuff has to do with Transformer part that is specific to TFGrid and how Transfuser does it
#Ignore for now - but may need to come back to see how it affects the overall architecture

# class ImageCNN(nn.Module): 
#     """ 
#     Encoder network for image input list.
#     Args:
#         c_dim (int): output dimension of the latent embedding
#         normalize (bool): whether the input images should be normalized
#     """

#     #This is the part of the image processing of the transformer part where it uses ResNet 34 on images
#     def __init__(self, c_dim, normalize=True):
#         super().__init__()
#         self.normalize = normalize
#         self.features = resnet34()
#         self.features.conv1 = nn.Conv2d(64, 64, kernel_size=7, stride=2, padding=3, bias=False)
#         self.features.fc = nn.Sequential()

#     def forward(self, inputs):
#         c = 0
#         for x in inputs:
#             if self.normalize:
#                 x = normalize_imagenet(x)
#             c += self.features(x)
#         return c

# def normalize_imagenet(x):
#     """ Normalize input images according to ImageNet standards.
#     Args:
#         x (tensor): input images
#     """
#     x = x.clone()
#     x[:, 0] = (x[:, 0] - 0.485) / 0.229
#     x[:, 1] = (x[:, 1] - 0.456) / 0.224
#     x[:, 2] = (x[:, 2] - 0.406) / 0.225
#     return x


# class LidarEncoder(nn.Module):
#     """
#     Encoder network for LiDAR input list
#     Args:
#         num_classes: output feature dimension
#         in_channels: input channels
#     """
#     #This is the part of the image processing of the transformer part where it uses ResNet 18 on lidar features
#     def __init__(self, num_classes=512, in_channels=64):
#         super().__init__()

#         self._model = resnet18()
        
#         self._model.fc = nn.Sequential()
#         _tmp = self._model.conv1
#         self._model.conv1 = nn.Conv2d(64, out_channels=_tmp.out_channels, 
#             kernel_size=_tmp.kernel_size, stride=_tmp.stride, padding=_tmp.padding, bias=_tmp.bias)

#     def forward(self, inputs):
#         features = 0
#         for lidar_data in inputs:
#             lidar_feature = self._model(lidar_data)
#             features += lidar_feature

#         return features


# class SelfAttention(nn.Module): #this is the self attention part of the transformer architecture in the middle of the overall architecture
#     """
#     A vanilla multi-head masked self-attention layer with a projection at the end.
#     """

#     def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop):
#         super().__init__()
#         assert n_embd % n_head == 0
#         # key, query, value projections for all heads
#         self.key = nn.Linear(n_embd, n_embd)
#         self.query = nn.Linear(n_embd, n_embd)
#         self.value = nn.Linear(n_embd, n_embd)
#         # regularization
#         self.attn_drop = nn.Dropout(attn_pdrop)
#         self.resid_drop = nn.Dropout(resid_pdrop)
#         # output projection
#         self.proj = nn.Linear(n_embd, n_embd)
#         self.n_head = n_head

#     def forward(self, x):
#         B, T, C = x.size()

#         # calculate query, key, values for all heads in batch and move head forward to be the batch dim
#         k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
#         q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
#         v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

#         # self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
#         att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
#         att = F.softmax(att, dim=-1)
#         att = self.attn_drop(att)
#         y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
#         y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

#         # output projection
#         y = self.resid_drop(self.proj(y))
#         return y


# class Block(nn.Module):
#     """ an unassuming Transformer block """
#  #Based on reading TFGrid - I think this is the bit where it gets the output feature based on the attention and the input features
#  #i.e. this is where F_o = f(Att) +F_i comes into play where f is an MLP, Att is the result of the Self-attention module and F_i is the features input to Self-Attention
#     def __init__(self, n_embd, n_head, block_exp, attn_pdrop, resid_pdrop):
#         super().__init__()
#         self.ln1 = nn.LayerNorm(n_embd)
#         self.ln2 = nn.LayerNorm(n_embd)
#         self.attn = SelfAttention(n_embd, n_head, attn_pdrop, resid_pdrop)
#         self.mlp = nn.Sequential(
#             nn.Linear(n_embd, block_exp * n_embd),
#             nn.ReLU(True), # changed from GELU
#             nn.Linear(block_exp * n_embd, n_embd),
#             nn.Dropout(resid_pdrop),
#         )

#     def forward(self, x):
#         B, T, C = x.size()

#         x = x + self.attn(self.ln1(x))
#         x = x + self.mlp(self.ln2(x))

#         return x

# #Not exactly sure if we need this - but is seems this is a class for representing the overall transformer
# #where the Block as described above
# class GPT(nn.Module):
#     """  the full GPT language model, with a context size of block_size """

#     def __init__(self, n_embd, n_head, block_exp, n_layer, 
#                     vert_anchors, horz_anchors, seq_len, 
#                     embd_pdrop, attn_pdrop, resid_pdrop, config):
#         super().__init__()
#         self.n_embd = n_embd
#         self.seq_len = seq_len
#         self.vert_anchors = vert_anchors
#         self.horz_anchors = horz_anchors
#         self.config = config

#         # positional embedding parameter (learnable), image + lidar
#         self.pos_emb = nn.Parameter(torch.zeros(1, (config['n_views'] + 1) * seq_len * vert_anchors * horz_anchors, n_embd))
        
#         # velocity embedding
#         self.vel_emb = nn.Linear(1, n_embd)
#         self.drop = nn.Dropout(embd_pdrop)

#         # transformer
#         self.blocks = nn.Sequential(*[Block(n_embd, n_head, 
#                         block_exp, attn_pdrop, resid_pdrop)
#                         for layer in range(n_layer)])
        
#         # decoder head
#         self.ln_f = nn.LayerNorm(n_embd)

#         self.block_size = seq_len
#         self.apply(self._init_weights)

#     def get_block_size(self):
#         return self.block_size

#     def _init_weights(self, module):
#         if isinstance(module, nn.Linear):
#             module.weight.data.normal_(mean=0.0, std=0.02)
#             if module.bias is not None:
#                 module.bias.data.zero_()
#         elif isinstance(module, nn.LayerNorm):
#             module.bias.data.zero_()
#             module.weight.data.fill_(1.0)

#     def configure_optimizers(self):
#         # separate out all parameters to those that will and won't experience regularizing weight decay
#         decay = set()
#         no_decay = set()
#         whitelist_weight_modules = (torch.nn.Linear, torch.nn.Conv2d)
#         blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.BatchNorm2d)
#         for mn, m in self.named_modules():
#             for pn, p in m.named_parameters():
#                 fpn = '%s.%s' % (mn, pn) if mn else pn # full param name

#                 if pn.endswith('bias'):
#                     # all biases will not be decayed
#                     no_decay.add(fpn)
#                 elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):
#                     # weights of whitelist modules will be weight decayed
#                     decay.add(fpn)
#                 elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):
#                     # weights of blacklist modules will NOT be weight decayed
#                     no_decay.add(fpn)

#         # special case the position embedding parameter in the root GPT module as not decayed
#         no_decay.add('pos_emb')

#         # create the pytorch optimizer object
#         param_dict = {pn: p for pn, p in self.named_parameters()}
#         optim_groups = [
#             {"params": [param_dict[pn] for pn in sorted(list(decay))], "weight_decay": 0.01},
#             {"params": [param_dict[pn] for pn in sorted(list(no_decay))], "weight_decay": 0.0},
#         ]

#         return optim_groups

#     def forward(self, image_tensor, lidar_tensor): #this is the forward function that runs the whole transformer block
#                                                    #image tensor and lidar tensor are the inputs
#                                                    #may need to understand exactly what B represents in terms of size of tensors - if this affects what I would use
#                                                    #if doing simple concatenation
#                                                    #not really sure what ego-velocity is or why we need it
#         """
#         Args:
#             image_tensor (tensor): B*4*seq_len, C, H, W
#             lidar_tensor (tensor): B*seq_len, C, H, W
#             velocity (tensor): ego-velocity
#         """
#         #print('0-lidar_tensor.shape: ', lidar_tensor.shape)#torch.Size([4, 64, 8, 8])        
#         #print('0-image_tensor.shape: ', image_tensor.shape)#torch.Size([4, 64, 8, 8])
#         #print('lidar_tensor.shape[2:4]: ', lidar_tensor.shape[2:4]) #torch.Size([8, 8])

#         bz = lidar_tensor.shape[0] // self.seq_len
#         h, w = lidar_tensor.shape[2:4]
        
#         # forward the image model for token embeddings
#         image_tensor = image_tensor.view(bz, self.seq_len, -1, h, w)
#         lidar_tensor = lidar_tensor.view(bz, self.seq_len, -1, h, w)

#         #print('1-lidar_tensor.shape: ', lidar_tensor.shape)#torch.Size([4, 1, 64, 8, 8])
#         #print('1-image_tensor.shape: ', image_tensor.shape)#torch.Size([4, 4, 16, 8, 8])    
        
#         # pad token embeddings along number of tokens dimension

#         token_embeddings = torch.cat([image_tensor, lidar_tensor], dim=1).permute(0,1,3,4,2).contiguous()
#         #print('token_embeddings.shape: ',token_embeddings.shape) torch.Size([4, 2, 8, 8, 64])
#         token_embeddings = token_embeddings.view(bz, -1, self.n_embd) # (B, an * T, C)
#         #print('token_embeddings.shape: ',token_embeddings.shape) torch.Size([4, 128, 64])
        
#         # project velocity to n_embed
#         #velocity_embeddings = self.vel_emb(velocity.unsqueeze(1)) # (B, C)

#         # add (learnable) positional embedding and velocity embedding for all tokens
#         x = self.drop(self.pos_emb + token_embeddings) #+ velocity_embeddings.unsqueeze(1)) # (B, an * T, C)
#         #print('drop x.shape: ',x.shape) torch.Size([4, 128, 64])

#         # x = self.drop(token_embeddings + velocity_embeddings.unsqueeze(1)) # (B, an * T, C)
#         x = self.blocks(x) # (B, an * T, C)
#         #print('blocks x.shape: ',x.shape) torch.Size([4, 128, 64])

#         x = self.ln_f(x) # (B, an * T, C)
        
#         #print('ln_f x.shape: ',x.shape) torch.Size([4, 128, 64])
#         #x = x.view(bz, (self.config.n_views + 1) * self.seq_len, self.vert_anchors, self.horz_anchors, self.n_embd)
#         x = x.view(bz, (self.seq_len + 1) * self.seq_len, self.vert_anchors, self.horz_anchors, self.n_embd)
#         #print('x.shape: ',x.shape) torch.Size([4, 2, 8, 8, 64])
        
#         x = x.permute(0,1,4,2,3).contiguous() # same as token_embeddings
#         #torch.Size([4, 2, 64, 8, 8])

#         image_tensor_out = x[:, :self.seq_len, :, :, :].contiguous().view(bz * self.seq_len, -1, h, w)
#         lidar_tensor_out = x[:, self.seq_len:, :, :, :].contiguous().view(bz * self.seq_len, -1, h, w)

#         return image_tensor_out, lidar_tensor_out


# class Transfuser(nn.Module): #Not exactly sure what GPT represents (the difference between it and Block)
#                              #but this Transfuser class seems to be where it is then repreated across scales
#                              #and the ConvTranspose operations are applied
#     """
#     Multi-scale Fusion Transformer for image + LiDAR feature fusion
#     """

#     def __init__(self, config):
#         super().__init__()
#         self.config = config
#         #changed from self.config.anchors - due to error - issue when calling attribute with . better tto use key name
#         #also below in all transformer 1,2,3,4 calls
#         self.avgpool = nn.AdaptiveAvgPool2d((config['vert_anchors'], config['horz_anchors'])) 
        
#         self.image_encoder = ImageCNN(512, normalize=True)
#         self.lidar_encoder = LidarEncoder(num_classes=512, in_channels=2)

#         self.transformer1 = GPT(n_embd=64,
#                             n_head=config['n_head'], 
#                             block_exp=config['block_exp'], 
#                             n_layer=config['n_layer'], 
#                             vert_anchors=config['vert_anchors'], 
#                             horz_anchors=config['horz_anchors'], 
#                             seq_len=config['seq_len'], 
#                             embd_pdrop=config['embd_pdrop'], 
#                             attn_pdrop=config['attn_pdrop'], 
#                             resid_pdrop=config['resid_pdrop'],
#                             config=config)
#         self.transformer2 = GPT(n_embd=128,
#                             n_head=config['n_head'], 
#                             block_exp=config['block_exp'], 
#                             n_layer=config['n_layer'], 
#                             vert_anchors=config['vert_anchors'], 
#                             horz_anchors=config['horz_anchors'], 
#                             seq_len=config['seq_len'], 
#                             embd_pdrop=config['embd_pdrop'], 
#                             attn_pdrop=config['attn_pdrop'], 
#                             resid_pdrop=config['resid_pdrop'],
#                             config=config)
#         self.transformer3 = GPT(n_embd=256,
#                             n_head=config['n_head'], 
#                             block_exp=config['block_exp'], 
#                             n_layer=config['n_layer'], 
#                             vert_anchors=config['vert_anchors'], 
#                             horz_anchors=config['horz_anchors'], 
#                             seq_len=config['seq_len'], 
#                             embd_pdrop=config['embd_pdrop'], 
#                             attn_pdrop=config['attn_pdrop'], 
#                             resid_pdrop=config['resid_pdrop'],
#                             config=config)
#         self.transformer4 = GPT(n_embd=512,
#                             n_head=config['n_head'], 
#                             block_exp=config['block_exp'], 
#                             n_layer=config['n_layer'], 
#                             vert_anchors=config['vert_anchors'], 
#                             horz_anchors=config['horz_anchors'], 
#                             seq_len=config['seq_len'], 
#                             embd_pdrop=config['embd_pdrop'], 
#                             attn_pdrop=config['attn_pdrop'], 
#                             resid_pdrop=config['resid_pdrop'],
#                             config=config)

#         self.up1 = nn.ConvTranspose2d(in_channels=64,
#                                     out_channels=64,
#                                     kernel_size =3,
#                                     stride=4
#                                     )

#         self.up2 = nn.ConvTranspose2d(in_channels=128,
#                                     out_channels=128,
#                                     kernel_size =3,
#                                     stride=8
#                                     )              
#         self.up3 = nn.ConvTranspose2d(in_channels=256,
#                                     out_channels=256,
#                                     kernel_size =3,
#                                     stride=16
#                                     )   
#         self.up4 = nn.ConvTranspose2d(in_channels=512,
#                                     out_channels=512,
#                                     kernel_size =3,
#                                     stride=32
#                                     )    

#         self.inChannels = 1920
#         self.concat_conv = nn.Sequential(
#             nn.Conv2d(self.inChannels, self.inChannels, kernel_size=3, padding=(1,1)),
#             nn.BatchNorm2d(self.inChannels),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(self.inChannels, self.inChannels, kernel_size=3, padding=(1,1)),
#             nn.BatchNorm2d(self.inChannels),
#             nn.ReLU(inplace=True)
#         )                                                                                             

        
#     def forward(self, image_list, lidar_list): #we then have the forward function to run all this forward and get the output fused_features 
#                                                #from the overall transformer part of the architecture - above we have it set up so there are 4 transformers
#                                                #in the models_2T file it is very very similar only there are just 2 transformers in the above part and that changes some sizes
#         '''
#         Image + LiDAR feature fusion using transformers
#         Args:
#             image_list (list): list of input images
#             lidar_list (list): list of input LiDAR BEV
#             velocity (tensor): input velocity from speedometer
#         '''
#         if self.image_encoder.normalize:
#             image_list = [normalize_imagenet(image_input) for image_input in image_list]
#         #len(image_list) #4
#         #image_list[0].shape) #torch.Size([64, 256, 256])

#         bz, lidar_channel, h, w = lidar_list.shape
#         img_channel = image_list[0].shape[0]           
#         self.config.n_views = len(image_list) // self.config.seq_len

#         image_tensor = torch.stack(image_list, dim=1).view(bz * self.config.seq_len, img_channel, h, w)
#         lidar_tensor = torch.stack(list(lidar_list), dim=1).view(bz * self.config.seq_len, lidar_channel, h, w)

#         #image_tensor.shape)#torch.Size([4, 64, 256, 256])
#         #lidar_tensor.shape)#torch.Size([4, 64, 256, 256])

#         image_features = self.image_encoder.features.conv1(image_tensor)
#         image_features = self.image_encoder.features.bn1(image_features)
#         image_features = self.image_encoder.features.relu(image_features)
#         image_features = self.image_encoder.features.maxpool(image_features)
#         lidar_features = self.lidar_encoder._model.conv1(lidar_tensor)
#         lidar_features = self.lidar_encoder._model.bn1(lidar_features)
#         lidar_features = self.lidar_encoder._model.relu(lidar_features)
#         lidar_features = self.lidar_encoder._model.maxpool(lidar_features)

#         image_features = self.image_encoder.features.layer1(image_features)
#         lidar_features = self.lidar_encoder._model.layer1(lidar_features)

#         #print('l-image_features: ',image_features.shape) torch.Size([4, 64, 64, 64])
#         #print('l-lidar_features: ',lidar_features.shape) torch.Size([4, 64, 64, 64])
#         # fusion at (B, 64, 64, 64)
#         image_embd_layer1 = self.avgpool(image_features)
#         lidar_embd_layer1 = self.avgpool(lidar_features)
#         image_features_layer1, lidar_features_layer1 = self.transformer1(image_embd_layer1, lidar_embd_layer1)
#         image_features_layer1 = F.interpolate(image_features_layer1, scale_factor=8, mode='bilinear')
#         lidar_features_layer1 = F.interpolate(lidar_features_layer1, scale_factor=8, mode='bilinear')
#         image_features = image_features + image_features_layer1
#         lidar_features = lidar_features + lidar_features_layer1

#         #print('1-image_features: ',image_features.shape)
#         #print('1-lidar_features: ',lidar_features.shape) torch.Size([4, 64, 64, 64])
#         ## add deconv

#         deConv1Img = self.up1(image_features, output_size = torch.Size([bz,img_channel, 256, 256]))
#         deConv1Points = self.up1(lidar_features, output_size = torch.Size([bz,lidar_channel, 256, 256]))

#         image_features = self.image_encoder.features.layer2(image_features)
#         lidar_features = self.lidar_encoder._model.layer2(lidar_features)
#         # fusion at (B, 128, 32, 32)
#         image_embd_layer2 = self.avgpool(image_features)
#         lidar_embd_layer2 = self.avgpool(lidar_features)
#         image_features_layer2, lidar_features_layer2 = self.transformer2(image_embd_layer2, lidar_embd_layer2)
#         image_features_layer2 = F.interpolate(image_features_layer2, scale_factor=4, mode='bilinear')
#         lidar_features_layer2 = F.interpolate(lidar_features_layer2, scale_factor=4, mode='bilinear')
#         image_features = image_features + image_features_layer2
#         lidar_features = lidar_features + lidar_features_layer2

#         #print('2-image_features: ',image_features.shape)
#         #print('2-lidar_features: ',lidar_features.shape)torch.Size([4, 128, 32, 32])

#         deConv2Img = self.up2(image_features, output_size = torch.Size([bz,img_channel, 256, 256]))
#         deConv2Points = self.up2(lidar_features, output_size = torch.Size([bz,lidar_channel, 256, 256]))

#         image_features = self.image_encoder.features.layer3(image_features)
#         lidar_features = self.lidar_encoder._model.layer3(lidar_features)
#         # fusion at (B, 256, 16, 16)
#         image_embd_layer3 = self.avgpool(image_features)
#         lidar_embd_layer3 = self.avgpool(lidar_features)
#         image_features_layer3, lidar_features_layer3 = self.transformer3(image_embd_layer3, lidar_embd_layer3)
#         image_features_layer3 = F.interpolate(image_features_layer3, scale_factor=2, mode='bilinear')
#         lidar_features_layer3 = F.interpolate(lidar_features_layer3, scale_factor=2, mode='bilinear')
#         image_features = image_features + image_features_layer3
#         lidar_features = lidar_features + lidar_features_layer3

#         #print('3-image_features: ',image_features.shape)
#         #print('3-lidar_features: ',lidar_features.shape)torch.Size([4, 256, 16, 16])

#         deConv3Img = self.up3(image_features, output_size = torch.Size([bz,img_channel, 256, 256]))
#         deConv3Points = self.up3(lidar_features, output_size = torch.Size([bz,lidar_channel, 256, 256]))

#         image_features = self.image_encoder.features.layer4(image_features)
#         lidar_features = self.lidar_encoder._model.layer4(lidar_features)
#         # fusion at (B, 512, 8, 8)
#         image_embd_layer4 = self.avgpool(image_features)
#         lidar_embd_layer4 = self.avgpool(lidar_features)
#         image_features_layer4, lidar_features_layer4 = self.transformer4(image_embd_layer4, lidar_embd_layer4)
#         image_features = image_features + image_features_layer4
#         lidar_features = lidar_features + lidar_features_layer4

#         #print('4-image_features: ',image_features.shape)
#         #print('4-lidar_features: ',lidar_features.shape) torch.Size([4, 512, 8, 8])

#         deConv4Img = self.up4(image_features, output_size = torch.Size([bz,img_channel, 256, 256]))
#         deConv4Points = self.up4(lidar_features, output_size = torch.Size([bz,lidar_channel, 256, 256]))

#         Img_fused_features = torch.cat([deConv1Img, deConv2Img,deConv3Img,deConv4Img], dim=1)
#         Points_fused_features = torch.cat([deConv1Points, deConv2Points,deConv3Points,deConv4Points], dim=1)

#         fused_features  = torch.cat([Img_fused_features, Points_fused_features], dim=1)
        
#         fused_features = self.concat_conv(fused_features)

#         return fused_features



###############################################
################ point pillars ################
###############################################

#NB THIS IS BASED ON THE POINTPILLARS PAPER - NEED TO UNDERSTAND THIS PROPERLY

class Empty(torch.nn.Module): #setting up an overall Empty class
    def __init__(self, *args, **kwargs):
        super(Empty, self).__init__()

    def forward(self, *args, **kwargs): #forward method for the Empty class
        if len(args) == 1:
            return args[0]
        elif len(args) == 0:
            return None
        return args


class PFNLayer(nn.Module): #This is the Pillar Feature Net Layer - in terms of feature encoding this is the important part
                           #after that in the PointPillars paper there is the 2D CNN backbone and then the detection head for 3D object detection
                           #but we really just want to encode the features 
    def __init__(self,
                 in_channels, #This is the number of input channels - (this number depends on whether we want there to be just one PFN layer or several)
                              #as below says, PointPillars only uses one - so input channels is D=9
                 out_channels, #number of output channels - this again is per layer - but if following PointPillars where there is just one layer
                               #I believe his should equal N - to create a D,P,N sized tensor
                 use_norm=True, #flag for whether to use Batch Norm or not
                 last_layer=False): #to indicate if it is the last_layer or not - if using multiple layers - set to False and then subsequently set to True below
                                    #in PillarFeatureNet class depending on how many such layers we want
        """
        Pillar Feature Net Layer.
        The Pillar Feature Net could be composed of a series of these layers, but the PointPillars paper results only
        used a single PFNLayer. This layer performs a similar role as second.pytorch.voxelnet.VFELayer.
        :param in_channels: <int>. Number of input channels.
        :param out_channels: <int>. Number of output channels.
        :param use_norm: <bool>. Whether to include BatchNorm.
        :param last_layer: <bool>. If last_layer, there is no concatenation of features.
        """

        super().__init__()
        self.name = 'PFNLayer' #layer name
        self.last_vfe = last_layer #bool to control if it is the last layer or not
        if not self.last_vfe:
            out_channels = out_channels // 2 #if not the last layer - set out_channels to half as Conv2D will half output size
        self.units = out_channels #self.units is then the number of output units we want in the linear MLP layer

        use_norm = True #assume we will generally want to use BatchNorm

        if use_norm: #since use_norm is True - this is the form the layer will take
            self.norm = nn.BatchNorm1d(self.units,eps=1e-3, momentum=0.01) #apply BatchNorm - see PyTroch documentation for def of eps and momentum
            self.linear = nn.Linear(in_channels, self.units,bias=False) #linear layer from input_channels to output_channels
        else:
            self.norm = Empty #if use_norm is False - just use the Empty class above
            self.linear = nn.Linear(in_channels, self.units,bias=True) #if not using use_norm=True then in the linear layer a bias is applied

    def forward(self, inputs): #this defines how to pass inputs forward through this layer

        x = self.linear(inputs) #linear layer given the inputs
        #NB this is used to permute the dimensions of the output - see PyTorch documentation for 'permute' - dimensions 1 and 2 flipped
        #Note - contiguous is used to return a contiguous in memory tensor with the same data as self tensor/return the self tensor if already in memory
        x = self.norm(x.permute(0, 2, 1).contiguous()).permute(0, 2, 1).contiguous() #Would be helpful to print sizes or try this with a simple example
        x = F.relu(x) #apply relu

        x_max = torch.max(x, dim=1, keepdim=True)[0] #NB This is where max operation over channels is applied to get an output sensor of size C,P
                                                     #Note when performing the max operation it will return values as index 0, indices of where these maxes occur as index 1,
                                                     #and we want the values so access index [0]

        if self.last_vfe:
            return x_max #return just this max tensor (dimensions (C,P)) if we are at the last layer
        else:
            x_repeat = x_max.repeat(1, inputs.shape[1], 1) #else just get repeated version of the x_max tensor - repeat inputs.shape[1] number of times across a row
            x_concatenated = torch.cat([x, x_repeat], dim=2) #concatenate it with the original x - then this gets passed to the next PFNLayer if we want multiple PFN layers
            return x_concatenated


class PillarFeatureNet(nn.Module): #This is then a class for creating the Pillar Feature Net by stacking PFN Layers (if we had multiple - for PointPillars we just want 1)
                                   #look at PyTorch documentation to understand what a module is
    def __init__(self,
                 num_input_features=4, #4 as we input point clouds with x,y,z,r information - then augment with further features based on pillar centres etc
                 use_norm=True, #want to use BatchNorm
                 num_filters=(64,), #this dictates the number of filters to use i.e. number of nodes in layers - here we have just one entry - so just 1 layer, as in PointPillars
                 with_distance=False,
                 voxel_size=(0.2, 0.2, 4), #NB these are the default voxel_size values - based on PointPillars paper for Car
                 pc_range=(0, -40, -3, 70.4, 40, 1)): #These are also based on the PointPillars paper - see where they mention the ,y,z range for Car
                                                      #not sure if this needs to change per class? - It is also different (but commented) in the config file
                                                      #so not sure which exactly to use
        """
        Pillar Feature Net.
        The network prepares the pillar features and performs forward pass through PFNLayers. This net performs a
        similar role to SECOND's second.pytorch.voxelnet.VoxelFeatureExtractor.
        :param num_input_features: <int>. Number of input features, either x, y, z or x, y, z, r.
        :param use_norm: <bool>. Whether to include BatchNorm.
        :param num_filters: (<int>: N). Number of features in each of the N PFNLayers.
        :param with_distance: <bool>. Whether to include Euclidean distance to points.
        :param voxel_size: (<float>: 3). Size of voxels, only utilize x and y size.
        :param pc_range: (<float>: 6). Point cloud range, only utilize x and y min.
        """

        super().__init__()
        self.name = 'PillarFeatureNet'
        assert len(num_filters) > 0 #this cannot be zero if num_filters is empty otherwise we won't be able to define the grid properly
        num_input_features += 5 #NB this is to do with the fact that we decorate with an additional 5 features - the arithmetic means for x,y,z and for x,y offsets to pilar centres
        if with_distance:
            num_input_features += 1 #if we want an additional feature for distance we can add one more feature (with_distance is set to False in the config file)
        self._with_distance = with_distance

        # Create PillarFeatureNet layers
        num_filters = [num_input_features] + list(num_filters) # [4,64] - this is a list of sizes - the number of elemts in the list will determine the number of layers
        pfn_layers = [] #an empty list to which we will append layers and then create a model from list of layers with nn.ModuleList below
        for i in range(len(num_filters) - 1): #incrementally add layers 
            in_filters = num_filters[i] #when i=0 and we are looking at the input layer with 4 inputs x,y,z,r
            out_filters = num_filters[i + 1] #output filters will be 64 i.e. this is C
            if i < len(num_filters) - 2: #if we want only 1 pfn layer - num_filters will have 2 elements so then len(num_filtes)-2 = 0 so we go to the else part
                                         #and set last_layer = True - so then we do the max pooling step in PFNLayer
                last_layer = False
            else:
                last_layer = True #set this to True so we can do the max pooling step
            pfn_layers.append(PFNLayer(in_filters, out_filters, use_norm, last_layer=last_layer)) #append a layer, based on these criteria to the list of layers
        self.pfn_layers = nn.ModuleList(pfn_layers) #once out of the for loop - assign self.pfn_layers based on the list - see PyTorch documentation on ModuleList

        # Need pillar (voxel) size and x/y offset in order to calculate pillar offset
        self.vx = voxel_size[0]
        self.vy = voxel_size[1]
        self.x_offset = self.vx / 2 + pc_range[0]
        self.y_offset = self.vy / 2 + pc_range[1]

    def forward(self, features, num_voxels, coors): #features [num_voxels, max_points, ndim] 
        #voxels from two batches are concatenated and coord have information corrd [num_voxels, (batch, x,y)]
        # pdb.set_trace()

        # Find distance of x, y, and z from cluster center
        #for last index of features (concerning ndims) go first the first 3 features i.e. indices 0,1,2 (stops before 3) - which represent x,y,z
        points_mean = features[:, :, :3].sum(dim=1, keepdim=True) / num_voxels.type_as(features).view(-1, 1, 1) 
        f_cluster = features[:, :, :3] - points_mean

        # Find distance of x, y, and z from pillar center
        f_center = torch.zeros_like(features[:, :, :2]) #zeros - only want ndims to now be for x and y (i.e. indicese 0 and 1 - stops before 2) in ndims dimension
                                                        #as we only calculate distance from pillar centre for x and y
        
        #NB - If you look through train.py it will point to points_to_voxel_loop in tools.py and that is where
        #coors is actually assigned - would need to check this to confirm if this is a typo or not
        #IT IS NOT A TYPO - points_to_voxel_loop points to points_to_voxel where there is a parameter reverse_index, set to True so it returns zyx, not xyz
        f_center[:, :, 0] = features[:, :, 0] - (coors[:, 3].float().unsqueeze(1) * self.vx + self.x_offset) 
        f_center[:, :, 1] = features[:, :, 1] - (coors[:, 2].float().unsqueeze(1) * self.vy + self.y_offset)

        # Combine together feature decorations
        features_ls = [features, f_cluster, f_center]
        if self._with_distance: #this is False in config file so generally this shouldn't apply - but if it is True - additionally add a distance feature
            points_dist = torch.norm(features[:, :, :3], 2, 2, keepdim=True)
            features_ls.append(points_dist)
        features = torch.cat(features_ls, dim=-1) #might want to check the size of features after this point

        # The feature decorations were calculated without regard to whether pillar was empty. Need to ensure that
        # empty pillars remain set to zeros.
        voxel_count = features.shape[1]
        mask = get_paddings_indicator(num_voxels, voxel_count, axis=0) #get paddings indicator is used to pad with zeros wherever we don't have enough data
        mask = torch.unsqueeze(mask, -1).type_as(features) #creates a mask to pad with zeros where there is empty data - unsqueeze(-1) essentially acts like transpose
        features *= mask #multiplies the features by this mask - a boolean mask so multiplying by it is equivalent to multiplying by 0 or 1

        # Forward pass through PFNLayers
        for pfn in self.pfn_layers:
            features = pfn(features) #here they are considering num of voxels as batch size for linear layer

        return features.squeeze() #squeeze back features after applying PFN layers


class PointPillarsScatter(nn.Module):
#NB - In the PointPillars paper it describes how features are "scattered back to the original pillar locations"
#that is what this class does - to return a pseudo-image of size C,H,W
    def __init__(self,
                 output_shape,
                 num_input_features=64):
        """
        Point Pillar's Scatter.
        Converts learned features from dense tensor to sparse pseudo image. This replaces SECOND's
        second.pytorch.voxelnet.SparseMiddleExtractor.
        :param output_shape: ([int]: 4). Required output shape of features.
        :param num_input_features: <int>. Number of input features.
        """

        super().__init__()
        self.name = 'PointPillarsScatter' #name this class PointPillarsScatter
        self.output_shape = output_shape #I believe the first index of the output shapre should be the batch_size
        self.ny = output_shape[2] #This is H if the output is of size C,H,W
        self.nx = output_shape[3] #This is W if the output is C,H,W
        self.nchannels = num_input_features #This is C in the notation of PointPillars

    def forward(self, voxel_features, coords, batch_size):

        # batch_canvas will be the final output.
        batch_canvas = []
        for batch_itt in range(batch_size):
            # Create the canvas for this sample
            canvas = torch.zeros(self.nchannels, self.nx * self.ny, dtype=voxel_features.dtype,
                                 device=voxel_features.device) #creates an array of zeros across nchannels - each a nx x ny grid - with the same data type as voxel_features

            # Only include non-empty pillars
            batch_mask = coords[:, 0] == batch_itt #as per above comments - coord is of form [num_voxels, (batch, x,y)] so match the batch index
            this_coords = coords[batch_mask, :] #Choosing coordinates and indices of those coordinatse according to which ones match the batch index
            indices = this_coords[:, 2] * self.nx + this_coords[:, 3]
            indices = indices.type(torch.long)
            voxels = voxel_features[batch_mask, :]
            voxels = voxels.t() #transpose - as xy grid coords are opposite of indices of a matrix i.e. you choose row then column

            # Now scatter the blob back to the canvas.
            canvas[:, indices] = voxels

            # Append to a list for later stacking.
            batch_canvas.append(canvas)

        # Stack to 3-dim tensor (batch-size, nchannels, nrows*ncols)
        batch_canvas = torch.stack(batch_canvas, 0)

        # Undo the column stacking to final 4-dim tensor
        batch_canvas = batch_canvas.view(batch_size, self.nchannels, self.ny, self.nx) #see pyTorch documentatio on view()

        return batch_canvas


def get_paddings_indicator(actual_num, max_num, axis=0): #this is used above for paddings the voxel features
    """Create boolean mask by actually number of a padded tensor.
    Args:
        actual_num ([type]): [description]
        max_num ([type]): [description]
    Returns:
        [type]: [description]
    """

    actual_num = torch.unsqueeze(actual_num, axis + 1)
    # tiled_actual_num: [N, M, 1]
    max_num_shape = [1] * len(actual_num.shape)
    max_num_shape[axis + 1] = -1
    max_num = torch.arange(
        max_num, dtype=torch.int, device=actual_num.device).view(max_num_shape)
    # tiled_actual_num: [[3,3,3,3,3], [4,4,4,4,4], [2,2,2,2,2]]
    # tiled_max_num: [[0,1,2,3,4], [0,1,2,3,4], [0,1,2,3,4]]
    paddings_indicator = actual_num.int() > max_num
    # paddings_indicator shape: [batch_size, max_num]
    return paddings_indicator


class PillarFeatures(nn.Module): #overall class for applying both the PillarFeatureNet and then scattering the features back to
                                 #original pillar locations to get a [batch_size, C,H,W] sized tensor

    def __init__(self, cfg):
        super(PillarFeatures, self).__init__()
        # voxel feature extractor
        self.cfg = cfg #feed in this cfg from the confif.py file - it will be cfg_pp when we come to using this PillarFeatures below
        self.voxel_feature_extractor = PillarFeatureNet( num_input_features = cfg['input_features'],
                use_norm = cfg['use_norm'],
                num_filters=cfg['vfe_filters'],
                with_distance=cfg['with_distance'],
                voxel_size=cfg['voxel_size'],
                pc_range=cfg['pc_range'])

        grid_size = (np.asarray(cfg['pc_range'][3:]) - np.asarray(cfg['pc_range'][:3])) / np.asarray(cfg['voxel_size']) #determine grid_size based on point cloud range and voxel size
        grid_size = np.round(grid_size).astype(np.int64) #rounding grid_size to integer values
        dense_shape = [1] + grid_size[::-1].tolist() + [cfg['vfe_filters'][-1]] #grid_size[::-1] reverses the index from xyz to zyx

        # Middle feature extractor
        self.middle_feature_extractor = PointPillarsScatter(output_shape = dense_shape,
                                        num_input_features = cfg['vfe_filters'][-1])        
    

    def forward(self, voxels, coors, num_points,bsz):

                  
        voxel_features = self.voxel_feature_extractor(voxels, num_points, coors)
        ### voxel_feature_extractor deliver pillar features
        
        spatial_features = self.middle_feature_extractor(voxel_features, coors, bsz)#self.cfg['batch_size']
        ### spatial_features (pseudo image based from Pillar features)

        return spatial_features #this will ultimately return the spatial features extracted by PointPillars, which we want of size [batch_size, C,H,W]
                                #This is what should then be taken for concatenation
                                #THIS IS WHAT WE NEED TO PRINT OUT/USE FOR CONCAT/SEE IF THIS LINES UP WITH WHAT Lift Splat GIVES

###############################################
############### Lift-splat ###################
###############################################

#This is used in the upsampling step when creating the BEV final semantic grid output
#However it is also used in the CamEncode part - as it relates to how Efficent Net works - still need to properly understand input and
#output sizes for that
#

class Up(nn.Module): 
    def __init__(self, in_channels, out_channels, scale_factor=2): #scale_factor controls by how much the outout is scaled up
        super().__init__()
        #see pyTorch documentation for Upsample - bilinear interpolation to be used for upsampling
        self.up = nn.Upsample(scale_factor=scale_factor, mode='bilinear',
                              align_corners=True)
        #convolutional block - convolution, BatchNorm, ReLU, repeated twice - 3x3 kernel with stride 1 will retain size - just different weights
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x1, x2): #When calling Up - this involves upsampling, concatenating with a second tensor, then convolving the resulting tensor
        x1 = self.up(x1)
        x1 = torch.cat([x2, x1], dim=1)
        return self.conv(x1)


class CamEncode(nn.Module): #NB This is the important part of the Lift step - that uses the Efficient Net b0 backbone
    def __init__(self, D, C, downsample):
        super(CamEncode, self).__init__()
        self.D = D #D - as per Lift Splat Shoot paper - this is the number of depth that we calculate features for - point cloud should be of size D.H.W
        self.C = C #C - as per Lift Splat Shoot paper - this is the number of features we predict when construction contect vectors

        self.trunk = EfficientNet.from_pretrained("efficientnet-b0") #use the pretrained EfficientNet-b0 weights

        self.up1 = Up(320+112, 512) #need to better understand why the number of input channels is 320+112 and why the number of output channels is 512
        self.depthnet = nn.Conv2d(512, self.D + self.C, kernel_size=1, padding=0) #this is the network that actually produces the alphas and context vectors

    def get_depth_dist(self, x, eps=1e-20): #apply softamx activation to get the depth distributions
        return x.softmax(dim=1)

    def get_depth_feat(self, x):
        x = self.get_eff_depth(x) #get the effective depths
        # Depth
        x = self.depthnet(x) ### generates features C(64) and alphas D, out D+C

        depth = self.get_depth_dist(x[:, :self.D]) ### applies softmax over alphas D(based on the number of bins for depths)
        #depth.unsqueeze(1) : , x[:, self.D:(self.D + self.C)].unsqueeze(2)  Column
        new_x = depth.unsqueeze(1) * x[:, self.D:(self.D + self.C)].unsqueeze(2) #alpha * C #this is the context vector scaled by alpha i.e. c_d in Eq 1 of Lift Splat Shoot


        return depth, new_x

    def get_eff_depth(self, x): #this is the bit where is actually applies EfficientNet to get the depths and context vectors - need to explore this GitHub more
        # adapted from https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/efficientnet_pytorch/model.py#L231
        endpoints = dict()

        # Stem
        x = self.trunk._swish(self.trunk._bn0(self.trunk._conv_stem(x)))
        prev_x = x

        # Blocks
        for idx, block in enumerate(self.trunk._blocks):
            drop_connect_rate = self.trunk._global_params.drop_connect_rate
            if drop_connect_rate:
                drop_connect_rate *= float(idx) / len(self.trunk._blocks) # scale drop connect_rate
            x = block(x, drop_connect_rate=drop_connect_rate)
            if prev_x.size(2) > x.size(2):
                endpoints['reduction_{}'.format(len(endpoints)+1)] = prev_x
            prev_x = x

        # Head
        endpoints['reduction_{}'.format(len(endpoints)+1)] = x
        x = self.up1(endpoints['reduction_5'], endpoints['reduction_4'])
        return x

    def forward(self, x):
        depth, x = self.get_depth_feat(x)
        return x #So ultimately the result of all of this CamEncode class are the camera features c_d - which we can then feed to the "Splat" step

#BevEncode_0 - this will be the ResNet 18 version
class BevEncode_0(nn.Module): #This class is used for the creation of BEV semantic grids - it should work on the fused features that result from
                            #either concatenation or feeding the camera and lidar features through the transformer
                            #NOTE IF I WANT TO DO SOMETHING DIFFERENT WITH THE ENCODER-DECODER PART OF THE NETWORK 0 I THINK THIS IS WHAT I WILL NEED TO TWEAK
                            #NEED TO BE VERY CAREFUL THOUGH - MESSING WITH SIZES AND NUMBER OF FILTERS ETC COULD RESULT IN ERRORS
    def __init__(self, inC, outC):
        super(BevEncode_0, self).__init__()

        trunk = resnet18(pretrained=False, zero_init_residual=True) #setting up the fact that Lift Splat Shoot uses a kernel of size 7, with stride 2 and padding
                                                                    #if inC is also 64, this effectively keeps the input and output size the same (see Sec 4.1 of Lift Splat Shoot)
        self.conv1 = nn.Conv2d(inC, 64, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = trunk.bn1 #follow this with BatchNorm
        self.relu = trunk.relu #follow this with ReLU

        self.layer1 = trunk.layer1 #Then use pretrained layers of ResNet 18 - layer1 - this is x1
        self.layer2 = trunk.layer2 #Layer 2 of pretrained ResNet18 - this is x2
        self.layer3 = trunk.layer3 #Layer 3 of pretrained ResNet18 - this is x3

        self.up1 = Up(64+256, 256, scale_factor=4) #need to figure out why the number of input channels needed is 64+256 - think it is because from ResNet 18 layer 1 has 
                                                   #64 channels, layer 3 has 256 and when you upsample x1 you don't change the number of channels you just change the
                                                   #dimensions of the image so it has the same height and width dimensions and 
                                                   #can be concatenated with x3, along the channel dimension, so then the number of channels is 64+256 and we then reduce that down to 256
                                                   #output channels after the 2 convolutions in the Up class
        self.up2 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='bilinear',
                              align_corners=True),
            nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, outC, kernel_size=1, padding=0), ##number of output channels outC
        )

    def forward(self, x): #this is the forward pass through the whole network for BEV encoding
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)

        x1 = self.layer1(x)
        x = self.layer2(x1)
        x = self.layer3(x)

        x = self.up1(x, x1)
        x = self.up2(x)

        return x

#BevEncode_1 - this will be the ResNet 34 version
class BevEncode_1(nn.Module): #This class is used for the creation of BEV semantic grids - it should work on the fused features that result from
                            #either concatenation or feeding the camera and lidar features through the transformer
                            #NOTE IF I WANT TO DO SOMETHING DIFFERENT WITH THE ENCODER-DECODER PART OF THE NETWORK 0 I THINK THIS IS WHAT I WILL NEED TO TWEAK
                            #NEED TO BE VERY CAREFUL THOUGH - MESSING WITH SIZES AND NUMBER OF FILTERS ETC COULD RESULT IN ERRORS
    def __init__(self, inC, outC):
        super(BevEncode_1, self).__init__()

        trunk = resnet34(pretrained=False, zero_init_residual=True) #setting up the fact that Lift Splat Shoot uses a kernel of size 7, with stride 2 and padding
                                                                    #if inC is also 64, this effectively keeps the input and output size the same (see Sec 4.1 of Lift Splat Shoot)
        self.conv1 = nn.Conv2d(inC, 64, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = trunk.bn1 #follow this with BatchNorm
        self.relu = trunk.relu #follow this with ReLU

        self.layer1 = trunk.layer1 #Then use pretrained layers of ResNet 18 - layer1 - this is x1
        self.layer2 = trunk.layer2 #Layer 2 of pretrained ResNet18 - this is x2
        self.layer3 = trunk.layer3 #Layer 3 of pretrained ResNet18 - this is x3

        self.up1 = Up(64+256, 256, scale_factor=4) #need to figure out why the number of input channels needed is 64+256 - think it is because from ResNet 18 layer 1 has 
                                                   #64 channels, layer 3 has 256 and when you upsample x1 you don't change the number of channels you just change the
                                                   #dimensions of the image so it has the same height and width dimensions and 
                                                   #can be concatenated with x3, along the channel dimension, so then the number of channels is 64+256 and we then reduce that down to 256
                                                   #output channels after the 2 convolutions in the Up class
        self.up2 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='bilinear',
                              align_corners=True),
            nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, outC, kernel_size=1, padding=0), ##number of output channels outC
        )

    def forward(self, x): #this is the forward pass through the whole network for BEV encoding
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)

        x1 = self.layer1(x)
        x = self.layer2(x1)
        x = self.layer3(x)

        x = self.up1(x, x1)
        x = self.up2(x)

        return x

#BevEncode_2 - this will be the ResNet 50 version
class BevEncode_2(nn.Module): #This class is used for the creation of BEV semantic grids - it should work on the fused features that result from
                            #either concatenation or feeding the camera and lidar features through the transformer
                            #NOTE IF I WANT TO DO SOMETHING DIFFERENT WITH THE ENCODER-DECODER PART OF THE NETWORK 0 I THINK THIS IS WHAT I WILL NEED TO TWEAK
                            #NEED TO BE VERY CAREFUL THOUGH - MESSING WITH SIZES AND NUMBER OF FILTERS ETC COULD RESULT IN ERRORS
    def __init__(self, inC, outC):
        super(BevEncode_2, self).__init__()

        trunk = resnet50(pretrained=False, zero_init_residual=True) #setting up the fact that Lift Splat Shoot uses a kernel of size 7, with stride 2 and padding
                                                                    #if inC is also 64, this effectively keeps the input and output size the same (see Sec 4.1 of Lift Splat Shoot)
        self.conv1 = nn.Conv2d(inC, 64, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = trunk.bn1 #follow this with BatchNorm
        self.relu = trunk.relu #follow this with ReLU

        self.layer1 = trunk.layer1 #Then use pretrained layers of ResNet 18 - layer1 - this is x1
        self.layer2 = trunk.layer2 #Layer 2 of pretrained ResNet18 - this is x2
        self.layer3 = trunk.layer3 #Layer 3 of pretrained ResNet18 - this is x3

        self.up1 = Up(256+1024, 1024, scale_factor=4) #need to figure out why the number of input channels needed is 64+256 - think it is because from ResNet 18 layer 1 has 
                                                   #64 channels, layer 3 has 256 and when you upsample x1 you don't change the number of channels you just change the
                                                   #dimensions of the image so it has the same height and width dimensions and 
                                                   #can be concatenated with x3, along the channel dimension, so then the number of channels is 64+256 and we then reduce that down to 256
                                                   #output channels after the 2 convolutions in the Up class
        self.up2 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='bilinear',
                              align_corners=True),
            nn.Conv2d(1024, 512, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, outC, kernel_size=1, padding=0), ##number of output channels outC
        )

    def forward(self, x): #this is the forward pass through the whole network for BEV encoding
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)

        x1 = self.layer1(x)
        x = self.layer2(x1)
        x = self.layer3(x)

        x = self.up1(x, x1)
        x = self.up2(x)

        return x

    
#This is where it all comes together - we instantiate this class when we call compile_model() below and then this is our end-to-end model with everything
#that we then go about training
#NOTE: Removed tf_config as this would not be relevant to pass if not using a transformer

class LPT(nn.Module):
    def __init__(self, grid_conf, data_aug_conf, outC,pp_config):
        super(LPT, self).__init__()
        self.grid_conf = grid_conf #this will come from the config file - this will govern how the final semantic grid is set up
        self.data_aug_conf = data_aug_conf #this will come from the config file and will determine how data augmentation is done
        self.pp_config = pp_config #this will come from the config file and will goven how PointPillars is implemnted

        #use gen_dx_bx from tools to set up the grid spacings
        dx, bx, nx = gen_dx_bx(self.grid_conf['xbound'],
                                              self.grid_conf['ybound'],
                                              self.grid_conf['zbound'],
                                              )
        #NB Need to introduce a new parameter 'encoder_mode' to determine whether resnet 18/34/50 blocks are being used in the encoder
        #0 -> resnet18
        #1 -> resnet34
        #2 -> resnet50
        encoder_mode = self.grid_conf['encoder_mode']
        
        self.dx = nn.Parameter(dx, requires_grad=False) #set these as parameters for the neural network
        self.bx = nn.Parameter(bx, requires_grad=False)
        self.nx = nn.Parameter(nx, requires_grad=False)

        self.downsample = 16 #parameter to control downsampling of the actual nuScenes images
        self.camC = 64 #parameter C that controls the size of the context vectors for Lift Splat Shoot
        self.frustum = self.create_frustum() #function defined below for creating frustum
        self.D, _, _, _ = self.frustum.shape #frustum shape
        self.camencode = CamEncode(self.D, self.camC, self.downsample) ## camC features from lift, D number of discrete bins for depths
        
        # sum lift-splat features and PointPillars features
        self.inFeatures = self.camC + self.pp_config['vfe_filters'][0] #why are these summed? where is this used?
        
        if encoder_mode == 0:
            self.bevencode = BevEncode_0(inC=128, outC=outC) ### outC number of output channels - why is inC equal to 1920? 
                                                              #It is different (=384 i.e. 256+64+64? when using 2 transformers
                                                              #see models_2t.py file - think this could be influenced by how the 
                                                              #transformers are influencing channels
                                                              #so want to use just concatenation - see if this affects channel depth)
                                                              #changed this in my code to 128 - conct along channel dim - 64+64
        elif encoder_mode == 1:
            self.bevencode = BevEncode_1(inC=128, outC=outC)
        elif encoder_mode == 2:
            self.bevencode = BevEncode_2(inC=128, outC=outC)
        else:
            raise ValueError("Error with encoder_mode value in grid_conf - value must be 0,1 or 2 - check config")
        
        self.pointpillars = PillarFeatures(pp_config) #instantiate PillarFeatures class

        #self.transfuser = Transfuser(tf_config) #instantiate transfuser class - I WILL NOT NEED THIS IF NOT USING TRANSFORMERS

        # toggle using QuickCumsum vs. autograd #NB Need to explore the alternative of how autograd is applied if not using QuickCumSum
        self.use_quickcumsum = True
    
    def create_frustum(self):
        # make grid in image plane
        ogfH, ogfW = self.data_aug_conf['final_dim'] #final height and width as per data augmentation configurations
        fH, fW = ogfH // self.downsample, ogfW // self.downsample #further downsample from those dimensions
        #NB When we pass *self.grid_conf['dbound'] the * when passed as an argument to a function causes the iterables (list, tuples) to be unpacked and passed to the function
        #dbound in the config file contains a list of 3 elements, so these are passed as start, end and step i.e. we go from 4m to 45m in steps of 1m as per Lift Splat Shoot
        #we then reshape the view and expand so the 2nd and 3rd dimensions are of size fH, fW 
        ds = torch.arange(*self.grid_conf['dbound'], dtype=torch.float).view(-1, 1, 1).expand(-1, fH, fW)        
        D, _, _ = ds.shape #as in Lift Splat Shoot paper we create a D.H.W sized point cloud for the camera features so this is how we assign D
        xs = torch.linspace(0, ogfW - 1, fW, dtype=torch.float).view(1, 1, fW).expand(D, fH, fW) #then create a linspace - starting fron 0 going to ogfW-1 inclusive, 
                                                                                                 #in fW evenly spaced steps - think with this every cell should be 16x16 pixels
                                                                                                 #if downsample is 16
        ys = torch.linspace(0, ogfH - 1, fH, dtype=torch.float).view(1, fH, 1).expand(D, fH, fW) #similarly create a linspace in Y direction
        print('xs shape: ', xs.shape)
        print('ys shape: ', ys.shape)
        # D x H x W x 3 #not sure why we have 3 separate spacings rather than just one DxHxW grid
        frustum = torch.stack((xs, ys, ds), -1) #stack xs,ys,ds
        return nn.Parameter(frustum, requires_grad=False)

    def get_geometry(self, rots, trans, intrins, post_rots, post_trans):
        """Determine the (x,y,z) locations (in the ego frame)
        of the points in the point cloud.
        Returns B x N x D x H/downsample x W/downsample x 3
        """
        B, N, _ = trans.shape ### B batch size, N number of cameras

        # undo post-transformation
        # B x N x D x H x W x 3
        points = self.frustum - post_trans.view(B, N, 1, 1, 1, 3) #reshape the post transformations (from ego to cam) - need to do this first to get right size
        points = torch.inverse(post_rots).view(B, N, 1, 1, 1, 3, 3).matmul(points.unsqueeze(-1)) #then invert the post rotations

        # cam_to_ego
        points = torch.cat((points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3],
                            points[:, :, :, :, :, 2:3]
                            ), 5)
        combine = rots.matmul(torch.inverse(intrins))
        points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)
        points += trans.view(B, N, 1, 1, 1, 3)

        return points

    def get_cam_feats(self, x):
        """Return B x N x D x H/downsample x W/downsample x C
        """
        B, N, C, imH, imW = x.shape

        x = x.view(B*N, C, imH, imW)
        x = self.camencode(x)
        x = x.view(B, N, self.camC, self.D, imH//self.downsample, imW//self.downsample)
        x = x.permute(0, 1, 3, 4, 5, 2)

        return x

    def voxel_pooling(self, geom_feats, x): #still don't fully understand what is going on here - this is the crux of the pooling step and what I would need to change if I want to
                                            #use BEVFusion's version of pooling
        B, N, D, H, W, C = x.shape
        Nprime = B*N*D*H*W

        # flatten x
        x = x.reshape(Nprime, C)

        # flatten indices
        geom_feats = ((geom_feats - (self.bx - self.dx/2.)) / self.dx).long()
        geom_feats = geom_feats.view(Nprime, 3)
        batch_ix = torch.cat([torch.full([Nprime//B, 1], ix,
                             device=x.device, dtype=torch.long) for ix in range(B)])
        geom_feats = torch.cat((geom_feats, batch_ix), 1)

        # filter out points that are outside box
        kept = (geom_feats[:, 0] >= 0) & (geom_feats[:, 0] < self.nx[0])\
            & (geom_feats[:, 1] >= 0) & (geom_feats[:, 1] < self.nx[1])\
            & (geom_feats[:, 2] >= 0) & (geom_feats[:, 2] < self.nx[2])
        x = x[kept]
        geom_feats = geom_feats[kept]

        # get tensors from the same voxel next to each other
        ranks = geom_feats[:, 0] * (self.nx[1] * self.nx[2] * B)\
            + geom_feats[:, 1] * (self.nx[2] * B)\
            + geom_feats[:, 2] * B\
            + geom_feats[:, 3]
        sorts = ranks.argsort()
        x, geom_feats, ranks = x[sorts], geom_feats[sorts], ranks[sorts] ### sort all points accoirdn to bin id (ranks are pillars?)

        # cumsum trick
        if not self.use_quickcumsum:
            x, geom_feats = cumsum_trick(x, geom_feats, ranks) ### "Cumulative sum pooling trick"
        else:
            x, geom_feats = QuickCumsum.apply(x, geom_feats, ranks)

        # griddify (B x C x Z x X x Y) ### nx:  tensor([200, 200,   1])  -- ( ,64,1,200,200)
        final = torch.zeros((B, C, self.nx[2], self.nx[0], self.nx[1]), device=x.device)
        final[geom_feats[:, 3], :, geom_feats[:, 2], geom_feats[:, 0], geom_feats[:, 1]] = x

        # collapse Z
        final = torch.cat(final.unbind(dim=2), 1) ### unbind removes tensor 2nd dimension (z) then dimension B*C*X*Y
                                                                                                               ##H*W?
        return final #NB THIS IS THE FINAL SIZE OF TENSOR THAT WE WANT - IF WE WERE TO CONCATENATE THE TENSORS RATHER THAN USE THE TRANSFORMER WE WOULD WANT TO TAKE IT FROM HERE

    def get_voxels(self, x, rots, trans, intrins, post_rots, post_trans): ### get pillars
        geom = self.get_geometry(rots, trans, intrins, post_rots, post_trans)
        x = self.get_cam_feats(x)

        x = self.voxel_pooling(geom, x) #### deliver tensor B*C*X*Y

        return x #THIS ESSENTIALLY COMBINES ALL THE STEPS ABOVE - THIS IS THE TENSOR WE WOULD WANT TO CONCATENATE WITH THE RESULT OF
                 #PillarFeatures that we get after calling self.pointpillars

    def forward(self, x, rots, trans, intrins, post_rots, post_trans, voxels, coors, num_points): #NB THIS IS WHERE WE PUT IT ALL TOGETHER
                                                                                                  #ACROSS THE ENTIRE ARCHITECTURE
        
        x = self.get_voxels(x, rots, trans, intrins, post_rots, post_trans) #this gives us the Lift Splat Shoot features - in shape (batch size, C,H,W)
        
        bsz = x.shape[0]
        pointpillars_features = self.pointpillars(voxels, coors, num_points,bsz) #this gives us the PointPillars features - in the form (batch size,C,H,W)
    
        #x = self.transfuser(x,pointpillars_features)#transfuser #THINK THIS IS WHERE WE WOULD NEED TO JUST REPLACE THE TRANSFUSER PART WITH CONCATENATION 
        x = torch.cat((x,pointpillars_features),dim=1) #want to concatenate along the channel dimension i.e C dimension
                                                       #NB PyTorch expects argument to be a sequence/tuple of tensors to be concatenated
        
        x = self.bevencode(x) ### encoder to create BEV - IF WE INSTEAD PASSED CONCATENATED FEATURES RATHER THAN THE RESULT OF TRANSFORMER
                                #IT MAY THEN BE OK TO USE BEVENCODE - BUT AGAIN - WE MAY WANT TO FIDDLE AROUND WITH THE WAY THE ENCODER-DECODER IS SET UP
        return x

#Note have removed tf_config from set of passed parameters as it would no longer be relevant - changed LPT definition
def compile_model(grid_conf, data_aug_conf, outC,cfg_pp): #NB THIS THE FINAL ULTIMATE FUNCTION WE WANT TO RUN IN SUBSEQUENT FILES!!!
    return LPT(grid_conf, data_aug_conf, outC,cfg_pp) #If we want to change LPT to not have the transformer bit - may need to rename the class - so we can return 
                                                                #something else