# -*- coding: utf-8 -*-
"""Commented data_py file from TFGrid.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QFViYZKBriO_RFC7j0buKQBOf6bTyEsO
"""

#This is just to initially download and decompress the mini version of nuScenes - everything below that is from the actual data.py file in TFGrid repo

# !mkdir -p /data/sets/nuscenes  # Make the directory to store the nuScenes dataset in.

# !wget https://www.nuscenes.org/data/v1.0-mini.tgz  # Download the nuScenes mini split.

# !tar -xf v1.0-mini.tgz -C /data/sets/nuscenes  # Uncompress the nuScenes mini split.

# !pip install nuscenes-devkit &> /dev/null  # Install nuScenes.

"""
Copyright (C) 2020 NVIDIA Corporation.  All rights reserved.
Licensed under the NVIDIA Source Code License. See LICENSE at https://github.com/nv-tlabs/lift-splat-shoot.
Authors: Jonah Philion and Sanja Fidler
"""

from tkinter import W #not sure what this is for - something to do with GUI but I don't see where we actually use it below?
import torch #NB - import pyTorch, definitely need this
import os #operating system to access file paths/directories etc
import numpy as np #obviously need numpy
from PIL import Image #need this for dealing with images - PIL = Python Imaging Library
import cv2 #CV2 for computer vision related tasks
from pyquaternion import Quaternion #for orientation - some nuScenes data will be stored as Quaternions - e.g. rotation for calibrated_sensor, sample_annotation, ego_pose
from nuscenes.nuscenes import NuScenes #importing the NuScenes class from nuscenes devkit
from nuscenes.utils.splits import create_splits_scenes #NB nuscenes utils folder - splits.
from nuscenes.utils.data_classes import Box #class within nuscenes/utils/data_classes.py file - Box is a class for 3D bounding boxes - need to see data_classes.py for more info
from glob import glob #for global variables

import torch.multiprocessing #think this is only necessary if training with multiple GPUs


from scipy.spatial import Delaunay #something to do with "Delaunay triangulation" - not sure why we need it

from .tools import get_lidar_data, img_transform, normalize_img, gen_dx_bx, read_point_cloud, get_gt_map_mask, get_nusc_maps

#Need to understand what all these do - within the tools folder of TFGrid repo - see this for full details.

#get_lidar_data(nusc, sample_rec, nsweeps, min_distance)

    # Returns at most nsweeps of lidar in the ego frame.
    # Returned tensor is 5(x, y, z, reflectance, dt) x N
    # Adapted from https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/utils/data_classes.py#L56
#Need to understand the choice of N - How do we decide how many lidar sweeps to include?

#img_transform - see tools.py in TFGrid repo
#This applies a transformation that resizes, rotates,flips the image and returns transformed image & post homography transformations for rotation & translation

#normalize_img - see tools.py in TFGrid repo
#A transformation defined in the NormalizeInverse class for normalising the image across colour channels, based on means, stds from ImageNet

#gen_dx_bx:
#Not exactly sure what dx and bx mean - but believe for now it has to do with the grid separation in the semantic grid produced
#see tools.py - dx and bx are based off of parameters xbound, ybound, zbound passed to gen_dx_bx

#read_point_cloud: NB - This is very important - this is what gives us our point cloud points as binary numpy format, of length 4

    # Loads LIDAR data from binary numpy format. Data is stored as (x, y, z, intensity, ring index).
    # :param pc_path: Path of the pointcloud file on disk.
    # :return: point cloud instance (x, y, z, reflectance).
#Look further into the read_point_cloud method in tools, but essentially this makes use of the fact that LidarPointCloud class in nuscenes/utils/data_classes.py
#has nbr_dims already defined to be 4 and from_file method for that class resizes the lidar readings to be of length 4

#get_gt_map_mask - NB This is for getting ground truth map mask i.e. the mask against which we compare in order to get IOU
#see tools.py for full details - just generally trying to understand what it is doing for now.
#Below you'll see the subtlety of matching train_labels with map_labels for grouping, say, all "vehicle" classes together

    # returns ground truth the mask for a class given a sample token
    # also plot mask or map if required.
    # nusc: Nuscenes object
    # nusc_map: NuScenes map object
    
    # sample_token: token from nuscenes sample
    # layer_name: Class name to retreive the mask as array
    # h_w: height and width for the mask area - NB can see in tools.py this is set t0 (100,100) as based on LSS paper ew got from -50m to 50m from ego car
                                              #in steps of 0.5m, so in total that's 100m in x, 100m in y and 200x200 cells in grid
    # canvas_size: canvas size to retreive the mask - In tools.py can see this is set to (200,200) as this is the number of cells in the resulting grid
    # retrun: mask from layer name with canvas size

#get_nusc_maps:
#Rather simple function that returns a dictionary of the various maps from nuScenes - keys are map names, values are the maps from nuScenes
#map_name keys are: "singapore-hollandvillage", "singapore-queenstown", "boston-seaport", "singapore-onenorth"

#This will be the class for which we define attributes and methods for our data
#NB The format will be a PyTorch dataset - need to look at the specifics of what this entails (see PyTorch documentation)
class NuscData(torch.utils.data.Dataset):
    def __init__(self, nusc, is_train, data_aug_conf, grid_conf, cfg_pp,nmap, train_label='vehicle', cond='' ):
        self.nusc = nusc #this is just defining an attribute that will point to the NuScenes class
        self.is_train = is_train #an attribute to say if it is training data or not
        self.data_aug_conf = data_aug_conf #This is a deictionary - properly defined within the TFGrid/src/config.py file - that specifies attributes around how the data
                                           #should be augmented - need to comment on that file to fully undertand it, but includes things like h=900, W=1600 which are the
                                           #heights and widths of images coming from nuScenes, as well as other parameters

        self.grid_conf = grid_conf #This is another dictionary - properly defined in the TFGrid/src/config.py file. It has to do with the specifics of the grid
                                   #believe this is the BEV grid that is produced as final output - it specifies xbound, ybound, zbound for how far the grid extends
                                   #and also dbound specifies the distribution of depths D (see LSS paper) for which we project features onto as part of the "lift" step

        self.n_points=cfg_pp['n_points'] #cfg_pp is a dictionary of configuration parameters for the PointPillars part of the network - one of the keys of which is n_points
                                         #not exactly sure right now what this is supposed to signify - need to revise this
        
        self.pc_range = cfg_pp['pc_range'] #another parameter associated with PointPillars setup - think it has to do with ranges of pillar centres
                                           #is commented out in the config.py file in any case so do we really need it?

        self.nusc_map = nmap #think this corresponds to the nuScenes map we are dealing with
        self.nr_conditions = cond #conditions - not sure where this is relevant

        if cfg_pp['num_classes'] == 1: #if dealing with only 1 class - set label from train_label passed
            self.train_label = train_label
        
        self.scenes = self.get_scenes() #scene split based on get_scenes method below
        self.ixes = self.prepro() #to get tokens of samples in the split

        dx, bx, nx = gen_dx_bx(grid_conf['xbound'], grid_conf['ybound'], grid_conf['zbound']) #use gen_dx_bx from tools, based on xbound, ybound, zbound
                                                                                              #from grid_conf dictionary to get the grid sizings
        
        self.dx, self.bx, self.nx = dx.numpy(), bx.numpy(), nx.numpy() #convert dx, bx, nx to numpy and store as attributes

        self.fix_nuscenes_formatting() #for fixing formatting - not sure if this will actually be needed.

        #print(self)
    
    #This is relatively self-explanatory for now and not commenting on this for underatanding as I don't think it is needed if file paths are OK 
    def fix_nuscenes_formatting(self):
        """If nuscenes is stored with trainval/1 trainval/2 ... structure, adjust the file paths
        stored in the nuScenes object.
        """
        # check if default file paths work
        rec = self.ixes[0]
        sampimg = self.nusc.get('sample_data', rec['data']['CAM_FRONT'])
        imgname = os.path.join(self.nusc.dataroot, sampimg['filename'])

        def find_name(f):
            d, fi = os.path.split(f)
            d, di = os.path.split(d)
            d, d0 = os.path.split(d)
            d, d1 = os.path.split(d)
            d, d2 = os.path.split(d)
            return di, fi, f'{d2}/{d1}/{d0}/{di}/{fi}'

        # adjust the image paths if needed
        if not os.path.isfile(imgname):
            print('adjusting nuscenes file paths')
            fs = glob(os.path.join(self.nusc.dataroot, 'samples/*/samples/CAM*/*.jpg'))
            fs += glob(os.path.join(self.nusc.dataroot, 'samples/*/samples/LIDAR_TOP/*.pcd.bin'))
            info = {}
            for f in fs:
                di, fi, fname = find_name(f)
                info[f'samples/{di}/{fi}'] = fname
            fs = glob(os.path.join(self.nusc.dataroot, 'sweeps/*/sweeps/LIDAR_TOP/*.pcd.bin'))
            for f in fs:
                di, fi, fname = find_name(f)
                info[f'sweeps/{di}/{fi}'] = fname
            for rec in self.nusc.sample_data:
                if rec['channel'] == 'LIDAR_TOP' or (rec['is_key_frame'] and rec['channel'] in self.data_aug_conf['cams']):
                    rec['filename'] = info[rec['filename']]

    
    def get_scenes(self):
        # filter by scene 
        #Create a dictionary called split -where the first key is based on version (allows for trainval and mini) - and the values are themselves dictionaries
        #where the keys (True/False) determine if it is a train or val split - pass the nusc.version parameter and self.is_train to access the appropriate split
        split = {
            'v1.0-trainval': {True: 'train', False: 'val'},
            'v1.0-mini': {True: 'mini_train', False: 'mini_val'},
        }[self.nusc.version][self.is_train]

        scenes = create_splits_scenes()[split] #create_splits_scenes - already imported from nuscenes/utils/split.py - splits the scenes based on split

        return scenes

    def prepro(self): #this is well commented and easy to follow
                      #we preprocess the samples to include only samples in the split (based on scenes)
                      #and if we have specified a night/rain condition we include samples only where the condition is in the scene description
                      #for the scene pointed to by the scene_token from the corresponding sample, otherwise get all samples
                      #Then sort samples by scene and timestamp - return all samples across the split
        samples = [samp for samp in self.nusc.sample]

        # remove samples that aren't in this split
        samples = [samp for samp in samples if
                   self.nusc.get('scene', samp['scene_token'])['name'] in self.scenes]

        #search for night or rain conditions
        if self.nr_conditions != '':
            assert self.nr_conditions in ['night','rain'], "Invalid weather condition"
            print('Getting samples ', self.nr_conditions)
            samples = [samp for samp in samples if       
                      self.nr_conditions in self.nusc.get('scene', samp['scene_token'])['description'].lower()]                   
        else:
            print('Getting all samples')

        # sort by scene, timestamp (only to make chronological viz easier)
        samples.sort(key=lambda x: (x['scene_token'], x['timestamp']))

        return samples
 #function for augmenting data from a sample -to be applied to just the images,only used below in get_image_data   
    def sample_augmentation(self):
        H, W = self.data_aug_conf['H'], self.data_aug_conf['W'] #this is size of images as captured in nuScenes - specified in the data_aug_conf dictionary
        fH, fW = self.data_aug_conf['final_dim'] #these are the final dims we want to resize to - 128x352 as specified in TFGrid/src/config.py
        if self.is_train: #if we are dealing with the training split
            resize = np.random.uniform(*self.data_aug_conf['resize_lim']) #choose random proportion (between 0 and 1) of the resize_lim - will determine by how much we resize img
            resize_dims = (int(W*resize), int(H*resize))
            newW, newH = resize_dims
            crop_h = int((1 - np.random.uniform(*self.data_aug_conf['bot_pct_lim']))*newH) - fH #not exactly sure of the purpose of bot_pct_lim but it is used to set crop_h
            crop_w = int(np.random.uniform(0, max(0, newW - fW))) #from this set crop_w - really these just seem to be dimensions for cropping
            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)
            flip = False
            if self.data_aug_conf['rand_flip'] and np.random.choice([0, 1]): #if we have specified a random flip - do a coin toss - if choice returns 1, flip the image
                flip = True
            rotate = np.random.uniform(*self.data_aug_conf['rot_lim']) #rotation as set as a propriton of the rotation limit
        else:
            resize = max(fH/H, fW/W) #always resize to be fH/H, fW/W fraction of the original size
            resize_dims = (int(W*resize), int(H*resize))
            newW, newH = resize_dims
            crop_h = int((1 - np.mean(self.data_aug_conf['bot_pct_lim']))*newH) - fH #still not 100% sure what this is doing
            crop_w = int(max(0, newW - fW) / 2)
            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)
            flip = False
            rotate = 0
        return resize, resize_dims, crop, flip, rotate

    def get_image_data(self, rec, cams):
        imgs = [] #list for storing images
        rots = [] #list for storing rotations
        trans = [] #list for storing translations
        intrins = [] #list for intrinsics
        post_rots = [] #post rotations
        post_trans = [] #post translations
        for cam in cams: #cams is a list of the cameras whos data we want to process - we specify ncams in config.py - in theory this should work for arbitrary num of cams
            samp = self.nusc.get('sample_data', rec['data'][cam]) #get the sample data for a particular recording, for a particular camera - by specifying
                                                                  #sample_data is coming from a camera THIS IS HOW WE SPLIT UP THAT WE WNAT IMAGES SPECIFICALLY
            imgname = os.path.join(self.nusc.dataroot, samp['filename']) #join filename to the dataroot i.e data/sets/nuscenes/(IMAGE FILEPATH HERE)
            img = Image.open(imgname) #open the image at the given filepath
            post_rot = torch.eye(2) #returns a 2x2 matrix with 1s on diagonal, zeros elsewhere
            post_tran = torch.zeros(2) #returns a 2x2 matrix filled with 0s

            sens = self.nusc.get('calibrated_sensor', samp['calibrated_sensor_token']) #get calibrated sensor info from corresponding calibrated sensor for that sample data
            intrin = torch.Tensor(sens['camera_intrinsic']) #return the intrinsic info from that sensor - convert to tensor (originally it is 3x3 float - see nuScenes docs)
            rot = torch.Tensor(Quaternion(sens['rotation']).rotation_matrix) #Get rotation of sensor as a quaternion, use Quaternion from pyquaternion, then the rotation_matrix
                                                                             #for the Quaternion class returns the equivalent 3x3 rotation matrix from the quaternion
                                                                             #then cast that as a pyTorch tensor

            tran = torch.Tensor(sens['translation']) #get the translation from the calibrated sensor - float of length 3, cast to pyTorch tensor

            # augmentation (resize, crop, horizontal flip, rotate)
            resize, resize_dims, crop, flip, rotate = self.sample_augmentation() #call augmentation as above
            img, post_rot2, post_tran2 = img_transform(img, post_rot, post_tran, #NB img_transform - this comes from tools.py
                                                     resize=resize,
                                                     resize_dims=resize_dims,
                                                     crop=crop,
                                                     flip=flip,
                                                     rotate=rotate,
                                                     )
            
            # for convenience, make augmentation matrices 3x3
            post_tran = torch.zeros(3)
            post_rot = torch.eye(3)
            post_tran[:2] = post_tran2
            post_rot[:2, :2] = post_rot2

            imgs.append(normalize_img(img)) #normalize_img also comes from tools - normalise based on means across channels - based on ImageNet - see comment above
            intrins.append(intrin)
            rots.append(rot)
            trans.append(tran)
            post_rots.append(post_rot)
            post_trans.append(post_tran)

        return (torch.stack(imgs), torch.stack(rots), torch.stack(trans),
                torch.stack(intrins), torch.stack(post_rots), torch.stack(post_trans)) #return stacked imgs, with their corresponding rotations, translations, intrinsics
                                                                                       #reverse rotations, and reverse translations
                                                                                       #NB - THE MAIN THING WE WANT OUT OF THIS IS THE IMAGES THEMSELVES

    def get_lidar_data(self, rec, nsweeps): #this uses the get_lidar_data method already defined in tools.py
                                            #which in turn draws on some code from nuscenes/utils/data_classes.py
                                            #eg. remove_close - removes points in point cloud within min_distance of the sensor - not sure where the value of 2.2 came from?
                                            #This method returns x,y,z,reflectance but we are only interested in x,y,z to train on - return this reduced data as a tensor
        pts = get_lidar_data(self.nusc, rec,
                       nsweeps=nsweeps, min_distance=2.2)
        return torch.Tensor(pts)[:3]  # x,y,z
    
    def random_sample_points(self, cloud, N):
    
        cloud = torch.from_numpy(np.asarray(cloud)).float()

        points_count = cloud.shape[0]
        
        if(points_count > 1):
            prob = torch.randperm(points_count) # sampling without replacement
            if(points_count > N):
                idx = prob[:N]
                sampled_cloud = cloud[idx]
                
            else:
                r = int(N/points_count)
                cloud = cloud.repeat(r+1,1)
                sampled_cloud = cloud[:N]

        else:
            sampled_cloud = torch.ones(N,3)
        
        return sampled_cloud#.cpu().numpy()  

    def in_hull(self,p, hull):
        
        if not isinstance(hull,Delaunay):
            hull = Delaunay(hull)
        return hull.find_simplex(p)>=0

    def extract_pc_in_box2d(self,pc):
        ''' pc: (N,2), box2d: (xmin,ymin,xmax,ymax) '''
        box2d =  [self.pc_range[0],
                   self.pc_range[1],
                   self.pc_range[3],
                   self.pc_range[4]]

        box2d_corners = np.zeros((4,2))
        box2d_corners[0,:] = [box2d[0],box2d[1]] 
        box2d_corners[1,:] = [box2d[2],box2d[1]] 
        box2d_corners[2,:] = [box2d[2],box2d[3]] 
        box2d_corners[3,:] = [box2d[0],box2d[3]] 
        box2d_roi_inds = self.in_hull(pc[:,0:2], box2d_corners)
        
        return pc[box2d_roi_inds,:]


    def get_point_cloud(self, rec):
        
        pts = read_point_cloud(self.nusc,rec)#torch.Tensor(get_lidar_data(self.nusc, rec, nsweeps=1, min_distance=0))[:4]#

        pts = self.extract_pc_in_box2d(pts)

        pts = self.random_sample_points(pts,self.n_points)

        return pts

    def get_binimg(self, rec):
        egopose = self.nusc.get('ego_pose',
                                self.nusc.get('sample_data', rec['data']['LIDAR_TOP'])['ego_pose_token'])
        trans = -np.array(egopose['translation'])
        rot = Quaternion(egopose['rotation']).inverse
        img = np.zeros((self.nx[0], self.nx[1]))

        map_labels = ['drivable_area','walkway','lane_divider','stop_line','ped_crossing']
        
        if self.train_label in map_labels:
            img = get_gt_map_mask(self.nusc, self.nusc_map, rec,[self.train_label],
                                  h = self.pc_range[4]*2,w = self.pc_range[4]*2,
                                  canvas_size = (self.nx[0], self.nx[1])).squeeze(axis=0)
        else:    
            for tok in rec['anns']:            
                inst = self.nusc.get('sample_annotation', tok)
                # add category for lyft
                if not inst['category_name'].split('.')[0] == self.train_label:
                    continue
                box = Box(inst['translation'], inst['size'], Quaternion(inst['rotation']))
                box.translate(trans)
                box.rotate(rot)

                pts = box.bottom_corners()[:2].T
                pts = np.round(
                    (pts - self.bx[:2] + self.dx[:2]/2.) / self.dx[:2]
                    ).astype(np.int32)
                pts[:, [1, 0]] = pts[:, [0, 1]]
                cv2.fillPoly(img, [pts], 1.0)

        return torch.Tensor(img).unsqueeze(0)

    def choose_cams(self):
        if self.is_train and self.data_aug_conf['Ncams'] < len(self.data_aug_conf['cams']):
            cams = np.random.choice(self.data_aug_conf['cams'], self.data_aug_conf['Ncams'],
                                    replace=False)
        else:
            cams = self.data_aug_conf['cams']
        return cams

    def __str__(self):
        return f"""NuscData: {len(self)} samples. Split: {"train" if self.is_train else "val"}.
                   Augmentation Conf: {self.data_aug_conf}"""

    def __len__(self):
        return len(self.ixes)


class VizData(NuscData):
    def __init__(self, *args, **kwargs):
        super(VizData, self).__init__(*args, **kwargs)
    
    def __getitem__(self, index):
        rec = self.ixes[index]
        
        cams = self.choose_cams()
        imgs, rots, trans, intrins, post_rots, post_trans = self.get_image_data(rec, cams)
        lidar_data = self.get_lidar_data(rec, nsweeps=3)
        binimg = self.get_binimg(rec)
        
        return imgs, rots, trans, intrins, post_rots, post_trans, lidar_data, binimg


class SegmentationData(NuscData):
    def __init__(self, *args, **kwargs):
        super(SegmentationData, self).__init__(*args, **kwargs)
    
    def __getitem__(self, index):
        rec = self.ixes[index]

        cams = self.choose_cams()
        imgs, rots, trans, intrins, post_rots, post_trans = self.get_image_data(rec, cams)
        binimg = self.get_binimg(rec)

        points = self.get_point_cloud(rec)
        
        return imgs, rots, trans, intrins, post_rots, post_trans, binimg, points


def worker_rnd_init(x):
    np.random.seed(13 + x)


def compile_data(version, dataroot, data_aug_conf, grid_conf, bsz,
                 nworkers, parser_name, cfg_pp,train_label,cond='', dist=False, rank = 0, sw = 1):
    nusc = NuScenes(version='v1.0-{}'.format(version),
                    dataroot=dataroot,
                    verbose=False)
    nusc_map = get_nusc_maps(dataroot)
    parser = {
        'vizdata': VizData,
        'segmentationdata': SegmentationData,
    }[parser_name]

    if cfg_pp['num_classes'] == 1:
        assert train_label in ['vehicle','drivable_area','walkway','lane_divider','human', 'ped_crossing','stop_line'], "Invalid class"

    traindata = parser(nusc, is_train=True, data_aug_conf=data_aug_conf,
                         grid_conf=grid_conf, cfg_pp=cfg_pp,nmap = nusc_map,
                         train_label = train_label,cond=cond)
    valdata = parser(nusc, is_train=False, data_aug_conf=data_aug_conf,
                       grid_conf=grid_conf,cfg_pp=cfg_pp,nmap = nusc_map,
                       train_label = train_label, cond = cond)

    train_sampler = torch.utils.data.distributed.DistributedSampler(
    	traindata,
    	num_replicas=sw,
    	rank=rank,
    )

    val_sampler = torch.utils.data.distributed.DistributedSampler(
    	valdata,
    	num_replicas=sw,
    	rank=rank,
    )

    if dist:
        trainloader = torch.utils.data.DataLoader(traindata, batch_size=bsz,
                                                shuffle=False,
                                                num_workers=nworkers,
                                                drop_last=True,
                                                worker_init_fn=worker_rnd_init,
                                                pin_memory=True,
                                                sampler=train_sampler)
        valloader = torch.utils.data.DataLoader(valdata, batch_size=bsz,
                                                shuffle=False,
                                                num_workers=nworkers,
                                                drop_last=True,
                                                pin_memory=True,
                                                sampler=val_sampler)
    else:

        trainloader = torch.utils.data.DataLoader(traindata, batch_size=bsz,
                                                shuffle=True,
                                                num_workers=nworkers,
                                                drop_last=True,
                                                worker_init_fn=worker_rnd_init)
        valloader = torch.utils.data.DataLoader(valdata, batch_size=bsz,
                                                shuffle=False,
                                                num_workers=nworkers,
                                                drop_last=True)

    return trainloader, valloader